# Python爬虫

#### 通常我们用爬虫总分为三步

1. 获取网页内容
2. 解析网页内容
3. 储存和分析数据

<hr>

<hr>

这里有网站的反爬机制一般有  

* 有些内容我们必须登录才能看到
* 或者有验证码等限制机器的机制
* 还有重定向

**这主要是我目前进行爬虫学习主要解决目的**，本次将以request库和Beautiful Soup库来进行讲解



### 在第一步获取网页内容

我们将学习http请求，通过发送http请求来获取网页的内容 **这里将进行对python中request库的学习**简单发送http请求

$$
http请求(超文本传输协议) 是一种客户端和服务器之间的请求-响应协议  比如浏览器就可以被看作客户端，当在浏览器地址栏输入网址，浏览器想该网站的服务器发送请求，然后等待服务器返回给浏览器响应。http中有不同的请求的方法，其中大部分用post和get这两种请求，get用于获得数据，post用于创建数据

一个完整的**http请求**由三部分组成     **请求行**：<u>*方法类型*</u>: post get 等方法   <u>*资源路径:</u>*（www.douban.com/movie/top250)这个网址中 / 表示资源路径的根，所以movie/top250就是资源路径.  *<u>协议版本</u>*: 各个版本号
​								   **请求头**：包含一些给服务器的信息 比如 *<u>host</u>* :  *<u>User-Agent</u>* :用来告知服务器客户端的相关信息(比如请求是浏览器发送出来的 还是其他发出来的，如果是浏览器的话 类型是什么 版本是什么)  *<u>Accept</u>*: 客户端想接收的数据类型是什么类型的
​                                  **请求体**：可以放入客户端传给服务器的其他任意数据   但是get请求中请求体一般是空的

一个完整的**http响应**由三部分组成      **状态行**: 包含了*<u>协议版本</u>*：各个版本号 *<u>状态码</u>* 和*<u>状态信息</u>* :

<img src="images/Python爬虫/屏幕截图 2023-10-14 235436.png" style="zoom: 33%;" />

                                    **响应头**: 包含一些告知客户端的信息  比如 **Date**生成响应的日期和时间  **Content--Type**:返回内容的类型及编码格式

​                                   **响应体** ：服务器想给客户端的数据内容
$$

```python
import requests
head = {"User-Agent":"Mozilla/5.0(Windows NT 10.0; Win64; x64)"}//这个作用是帮我们把爬虫程序伪装成正常的浏览器
response= requests.get("http://books.toscrape.com/",header head)
print(response)// response其实是Response的实例，代表服务器发送给我们的响应
print(response.status_code)//http状态码
print(response.text)///返回响应体的内容
```






### 在解析网页内容

根据前面的html学习笔记中进行复习和补充  因为发送请求后获得的网页内容主要是html的格式 

**解析方式一**：re解析（结合linux讲解的正则表达和工具OSCHINA）

**解析方式二**：bs4解析

**解析方式三**：Xpath解析

``` python
import re


# findall函数：匹配字符串中所有符合正则的内容。第一个参数是正则表达式，第二个参数是匹配内容，该函数返回类型是一个列表类型,所以当匹配内容多的话，效率不高
content = re.findall(r"\d+","我的电话号码是14709543011") 
print(conte)
# finditer 匹配字符串中所有内容，返回迭代器，然后从迭代器中拿到内容需要函数group()
content = re.finditer(r"\d+","my phonenumble:14709543011")
for id in content:
    print(id.group())
    
# 预加载正则表达式，返回的对象表示我们正则表达式的对象，该对象可以调用findall,finditer方法，其返回类型也就不一样
obj = re.compile(r"\w+.")
content = obj.finditer("545wefew")
for fd in content:
    print(fd.group())

    
# (?P<分组名称>正则表达)，可以简单理解一下就是我们想要的数据用括号括起来
s = """
<div class ='re解析'><span id = '123456'>数据解析</span></div>
<div class ='re解'><span id = '123456'>数据</span></div>
"""
obj = re.compile(r"<div class ='(?P<id>.*?)'><span id = '(?P<other>.*?)'>(?P<kdf>.*?)</span></div>" , re.S)
content = re.finditer(obj,s)
for fd in content:
    print(fd.group("kdf"))
    print(fd.group("id"))
    print(fd.group("other"))
    
    
    
    
#以下代码是显示了我们用request库和re解析库的源代码
import requests
import re
import csv

url = "https://movie.douban.com/chart"
header ={ "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.69"}
response = requests.get(url,headers=header)
Html_content = response.text
obj = re.compile(r' <table width="100%" class="">.*?<a class="nbg" href=".*?"  title="(?P<title>.*?)">.*?'
                 r'<div class="star clearfix">.*?<span class="pl">((?P<popular>.*?))</span>.*?'
                 r'</table>'
                 ,re.S)

#我们想要把爬到的数据放到文件里，这里我们用cvs库，也用到了csv库将我们爬到的数据放到.csv文件里
import requests
import re
import csv

url = "https://movie.douban.com/chart"
header ={ "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.69"}
response = requests.get(url,headers=header)
Html_content = response.text
obj = re.compile(r' <table width="100%" class="">.*?<a class="nbg" href=".*?"  title="(?P<title>.*?)">.*?'
                 r'<div class="star clearfix">.*?<span class="pl">((?P<popular>.*?))</span>.*?'
                 r'</table>'
                 ,re.S)

f = open("data.csv",mode="w",encoding="utf-8")
csvWrite = csv.writer(f)
dfg = re.finditer(obj,Html_content)
for bg in dfg:
    dic = bg.groupdict()
    csvWrite.writerow(dic.values())
print("over")

```



```python

#bs4解析 主要是根据html标签与属性jing'xin
import requests
from bs4 import BeautifulSoup
head = {"User-Agent":"Mozilla/5.0(Windows NT 10.0; Win64; x64)"}
content = requests.get("http://books.toscrape.com/",head).text
soup = BeautifulSoup(content,"html.parser") #第二个参数表示我们解析的是html源码
items = soup.findAll("h3") #这个意思是我们找到网页下所有的h3标签
for item in items:
	link = item.find("a") #在h3标签下a的标签
	print(link.string)#提取a标签下的字符串
    
    
    
    
import requests
from bs4 import BeautifulSoup

heade = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36 Edg/117.0.2045.60"}
response = requests.get("https://movie.douban.com/top250", headers=heade)

html = response.text
soup = BeautifulSoup(html,"html.parser")

items = soup.findAll("p",attrs={"class":""})
for item in items:
    print(item.text)
    
    
    
#这里用Bs4库爬取优美图库的图片
import requests
import time
from bs4 import BeautifulSoup


for i in range(1,3):#爬取1到3页的图片，自动跳转页数
    url = "http://www.umeituku.com/weimeitupian/{}.htm".format(i)
    responce = requests.get(url)
    responce.encoding = "unicode"#当我们输出网址源代码（print(responce.text)）时会有乱码，所以这里改一下服务器返回给我们的编码形式
    min_as = BeautifulSoup(responce.text,"html.parser")
    gf = min_as.find("div", attrs={"class":"TypeList"}).find_all("a")
    for k in gf:
        zi_url = k.get("href")#直接通过get拿到属性的值
    #拿到子页面的源代码
        resp = requests.get(zi_url)
        lkj = BeautifulSoup(resp.text,"html.parser")
        hgf = lkj.find("div",attrs={"class":"ImageBody"}).find("a")
        for vf in hgf:
            picture_name = vf.get("src")
        #向服务器直接请求picture_name，以便我们可以获得图片
            img_res = requests.get(picture_name)
        # 下载图片
            con_na = picture_name.split("/")[-1]
            with open("图片/"+con_na,mode="wb") as f:
            # img_res.content 拿到图片的字节，字节组成的内容就是一张图片
                f.write(img_res.content)
                print("over",con_na)
                time.sleep(1)
print("All over!")


```





另一种方式演示（requests 和 Xpath库）爬取京东网页

``` python

#request发起网络请求
#xpath进行数据解析

import  requests
from lxml import  etree



header={
    'User-Agent' :'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.46'
}
# proxy={
#     'http':'http://120.220.220.95:8085'
# }
def parse_html(html):
    root=etree.HTML(html)
    divs= root.xpath('//*[@id="J_goodsList"]/ul/li/div') # list[<Element>, ..]
    print(divs)
    for div in divs:
        title=div.xpath('/html/body/div/div/div/div/div/div/ul/li/div/div/a/em/text()')
        popular = div.xpath('//*[@id="J_goodsList"]/ul/li/div/div/strong/i/text()')
    print(title)
    print(popular)



def  get_request(url):
    resp=requests.get(url,headers=header)
    if resp.status_code == 200:
        parse_html(resp.text)
    else:
        print("请求失败！")





if __name__ == '__main__':
    get_request('https://search.jd.com/Search?keyword=%E6%89%8B%E6%9C%BA&wq=%E6%89%8B%E6%9C%BA')
```

###### 该方法的细节与解析

* 这段代码是一个简单的爬虫程序，用于从京东网站上提取手机商品的标题和受欢迎程度。它使用了`requests`库发送HTTP请求，并使用`lxml`库进行HTML解析。

* 代码中定义了一个`parse_html`函数，用于解析HTML内容。它首先使用XPath选择器`//*[@id="J_goodsList"]/ul/li/div`获取所有商品的`div`元素。然后，通过迭代这些`div`元素，使用XPath表达式`/html/body/div/div/div/div/div/div/ul/li/div/div/a/em/text()`提取每个商品的标题，以及`//*[@id="J_goodsList"]/ul/li/div/div/strong/i/text()`提取每个商品的受欢迎程度。

* `get_request`函数用于发送HTTP请求，并调用`parse_html`函数进行解析。在`if __name__ == '__main__':`部分，它调用`get_request`函数来获取京东手机商品列表页的内容。

* 请注意，代码中还包含了一个请求头`header`，用于模拟浏览器发送请求，并且有一个被注释掉的代理设置`proxy`，如果需要使用代理可以取消注释并填写正确的代理地址。

	<hr>

<hr>

<hr>
request知识补缺

1. 模拟浏览器处理cookie

   就是针对我们想要一些需要我们登录网站才能进行爬取数据，当我们登录进去后，要拿到服务器返回给我们的cookie，然后带着cookie去给服务器请求，操作必须连一块

   ``` python
   #我们适用session(回话)进行请求 -> session可以认为是一连串的请求,在这个过程中cookie不会丢失
   import request
   
   session = request.session()
   data = {
       "liginname":"....."
       "password":"....."
   }
   #首先获得request url 通常在检查里的network刷新后出来复制
   url = "我们复制的请求地址"
   session.post(url,data= data)
   #检查一下登录是否成功
   ##print(resp.text)
   #然后我们主要是要cookie 就用resp.cookie
   #因为上面的session中已经有我们登录的信息cookie，拿着这个session去访问我们想要爬取东西的url
   resp = session.get("这里是我们想爬取信息的url")
   print(resp.text)
   #以上的方法还可以以这种形式表示
   resp = requests.get("url地址",header= {
       "cookie":"复制粘贴cookie内容"
   })
   #这种
   
   
   ```

   

2. 防盗链处理  抓取梨数据

   ``` python
   ```

   

3. 代理  防止封ip

   ``` python
   ```

   








### 储存和分析数据

这由我们用户自行定义 对数据进行分析

这里以上面用了Requests库和Xpath库的代码演示，将我们爬取到的数据写进.txt文件中

``` python
import requests
from lxml import etree

header = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.46'
}


def parse_html(html):
    root = etree.HTML(html)
    divs = root.xpath('//*[@id="J_goodsList"]/ul/li/div')
    results = []  # 用于存储爬取到的数据
    for div in divs:
        title = div.xpath('/html/body/div/div/div/div/div/div/ul/li/div/div/a/em/text()')
        popular = div.xpath('//*[@id="J_goodsList"]/ul/li/div/div/strong/i/text()')
        # 将数据添加到结果列表中
        results.append({'title': title, 'popular': popular})

    return results


def get_request(url):
    resp = requests.get(url, headers=header)
    if resp.status_code == 200:
        return resp.text
    else:
        print("请求失败！")
        return None


def write_to_file(data, filename):
    with open(filename, 'w', encoding='utf-8') as file:
        for item in data:
            file.write(f'Title: {item["title"]}\n')
            file.write(f'Popular: {item["popular"]}\n')
            file.write('\n')


if __name__ == '__main__':
    url = 'https://search.jd.com/Search?keyword=%E6%89%8B%E6%9C%BA&enc=utf-8&wq=%E6%89%8B%E6%9C%BA'
    html = get_request(url)
    if html:
        data = parse_html(html)
        write_to_file(data, 'result1.txt')
```



#### 接下来介绍对数据进行清洗

这里用到了python中的Pandas库

Pandas是Python中一个非常流行的数据处理库，它提供了一些强大的数据结构和数据分析工具，可以帮助我们更方便、快捷地处理数据

* Pandas的使用

  主要介绍Pandas中的两种数据结构

  1.  `Series` 是一种类似于一维数组的对象，它由一组`数据`（各种Numpy数据类型）以及一组与之相关的数据标签（即`索引`）组成。

     ``` python
     import Pandas as pd
     	
     data = {'b':1,'a':2,'c':3,'d':4}
     ser1 = pd.Series(data)
     prite(ser1)
     
     #输出结果为
     # b   1
     # a   2
     # c   3
     # d   4
     #当没有传递index参数时，Series的索引为字典的key，索引对应的值为字典的value。
     ##################################################################
     ##################################################################
     
     ata = {'b': 1, 'a': 0, 'c': 2}
     index = ['b', 'c', 'd', 'a']
     ser1 = pd.Series(data=data,index=index)
     print(ser1)
     
     #输出结果为
     b    1.0
     c    2.0
     d    NaN
     a    0.0
     
         
     ```

  2. DataFrame`是一个`表格型的数据结构`，它含有一组`有序的列`，每列可以是不同的值类型（数值、字符串、布尔型值）。DataFrame 既有`行索引`也有`列索引`，它可以被看做由`Series 组成的字典`（共同用一个索引）。

     ``` python
     import Pandas as pd
     d = {
     "one": pd.Series([1.0, 2.0, 3.0], index=["a", "b", "c"]),
     "two": pd.Series([1.0, 2.0, 3.0, 4.0], index=["a", "b", "c", "d"])
     }
     ​
     # 没有传递索引和列，则结果的索引为各个Series索引的并集，列是字典的键
     df = pd.DataFrame(d)
     print(df)
     ​
     # 指定index，Series中匹配标签的数据会被取出，没有匹配的标签的值为NaN
     df = pd.DataFrame(d, index=["d", "b", "a"])
     print(df)
     ​
     # 同时指定了索引和列，同样的，如果字典中没有和指定列标签匹配的键，则结果中该列标签对应的列值都为NaN
     df = pd.DataFrame(d, index=["d", "b", "a"], columns=["two", "three"])
     print(df)
     ​
     #输出结果为：
        one  two
     a  1.0  1.0
     b  2.0  2.0
     c  3.0  3.0
     d  NaN  4.0
        one  two
     d  NaN  4.0
     b  2.0  2.0
     a  1.0  1.0
        two three
     d  4.0   NaN
     b  2.0   NaN
     a  1.0   NaN
     
     ```

     







<hr>

<hr>

<hr>

## 上述我们提到一般网站反爬机制有

* 有些内容我们必须登录才能看到
* 或者有验证码等限制机器的机制
* 还有重定向

### 接下来我们将解决上述该问题







###